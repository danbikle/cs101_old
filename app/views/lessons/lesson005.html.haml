%h1#top Lesson 005 [ Distributed Python with GCP Pub/Sub ]

%h2 Introduction

%p This lesson shows how I used GCP Pub/Sub to coordinate a Python script.

%p The script is named workon_tkrs.py.

%p The script was deployed across a GCP-Kubernetes cluster.

%p Deployment was done using the GCP web-UI and ssh.

%h2 Inspiration

%p This lesson is inspired by a Will Crichton and his github repo:

%p
  %a(href='https://github.com/willcrichton/gcp-job-queue' target='x')
    https://github.com/willcrichton/gcp-job-queue
  
%p
  The repo leads to a nice description of Pub/Sub, and other GCP topics:

%p
  %a(href='http://willcrichton.net/notes/gcp-job-queue/' target='x')
    http://willcrichton.net/notes/gcp-job-queue/

%h2 Create GCP Account

%p If you want to follow the steps listed in this lab, you will need to create a GCP account.

%p A useful URL is listed below:

%p
  %a(href='https://console.cloud.google.com/freetrial' target='x')
    https://console.cloud.google.com/freetrial
%p After you create a GCP account, you should login to it.

%p Then, you should create a project.

%p When I did that, I created a project named: "pubsub"

%p When I create a project I use a short name with only lower-case letters.

%p After you create a project, you should "upgrade" your account to gain extra GCP privileges.

%p I learned about "upgrade" while following a lab in Feb-2018:
%p
  %a(href='https://codelabs.developers.google.com/codelabs/cpb100-free-trial/#3' target='x')
    https://codelabs.developers.google.com/codelabs/cpb100-free-trial/#3
    
%p After you have a GCP account which has been "upgraded", you will be on a good path.


%h2 Part 1:
%p
  I list below steps I followed to create a simple GCP job-queue:

%ul
  %li
    Login to GCP:
    %a(href='https://console.cloud.google.com' target='x')
      https://console.cloud.google.com
  %li I saw this:
  %li
    %img(src='/lesson005/z001.png')
  %li I noticed at the top of the page that I was inside the "pubsub" project.
  %li During the time I have spent with GCP, I learned that I must have a sharp awareness of my projects.
  %li Eventually I gained a sense of how GCP pages are affected by projects.
  %li Initially I ignored projects which led me to some confusing situations.
  %li Enough about projects.
  %li
    I created a bucket: pubsub611
    %br/
    %a(href='https://console.cloud.google.com/storage' target='x')
      https://console.cloud.google.com/storage
  %li I saw this:
  %li
    %img(src='/lesson005/z002.png')


%h2 Part 2:
%ul
  %li
    At
    %a(href='https://console.cloud.google.com/iam-admin/serviceaccounts' target='x')
      https://console.cloud.google.com/iam-admin/serviceaccounts
  %li
    I created a service account: "srvacct":
  %li I saw this:
  %li
    %img(src='/lesson005/z003.png')

  %li
    Create service-key.json:
    %a(href='https://console.cloud.google.com/iam-admin/serviceaccounts' target='x')
      https://console.cloud.google.com/iam-admin/serviceaccounts
  %li
    click-permissions, select role: pub/sub-admin, generate a key:
    %br/
    \~/Downloads/pubsub-e71b0ed6e643.json
    copy downloaded-file to service-key.json
  %li I did that by starting here:
  %li
    %img(src='/lesson005/z004.png')
  
%h2 Part 3:
%ul
  %li
    Make srvacct an admin of pubsub611 bucket:
  %li
    %a(href='https://console.cloud.google.com/storage' target='x')
      https://console.cloud.google.com/storage

  %li I did that by starting here:
  %li
    %img(src='/lesson005/z005.png')

%h2 Part 4:
%ul
  %li
    I Created cluster cluster10,
    I made it Ubuntu with 3.75 gb ram, I ensured it has full access to cloud-APIs.
    %br
    %a(href='https://console.cloud.google.com/kubernetes' target='x')
      https://console.cloud.google.com/kubernetes
  %li I captured screen shots:
  %li
    %img(src='/lesson005/z006.png')
  %li
    %img(src='/lesson005/z007.png')
  %li
    %img(src='/lesson005/z008.png')
  %li
    I added 1 worker, Ubuntu:
  %li
    %img(src='/lesson005/z009.png')
  %li
    %img(src='/lesson005/z010.png')
  %li
    %img(src='/lesson005/z011.png')
  %li
    %img(src='/lesson005/z012.png')
    
  %li
    When done, I read IP addresses from here:
  %li
    %a(href='https://console.cloud.google.com/compute' target='x')
      https://console.cloud.google.com/compute
  %li
    %img(src='/lesson005/z013.png')
%h2 Part 5:
%ul
  %li
    On laptop I did this:
    %br
    vi .ssh/config
    .syntax
      %pre
        =render 'lesson005a'
%h2 Part 6:
%ul
  %li ssh into both mgr and worker
  %li
    On both hosts, I installed Python 3 via Anaconda:
  %li
    .syntax
      %pre
        %code
          sudo apt-get install -y bzip2
          wget https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh
          bash Anaconda3-5.1.0-Linux-x86_64.sh -b
          echo PATH="${HOME}/anaconda3/bin:$PATH" >> ${HOME}/.bashrc
          mv ~/anaconda3/bin/curl ~/anaconda3/bin/curl_ana
          bash
  %li
    On both hosts, I installed gcloud:
    .syntax
      %pre
        =render 'lesson005gci'

  %li
    I copied service-key.json from my laptop to both hosts:
    .syntax
      %pre
        %code
          scp ~/Downloads/service-key.json mgr:
          scp ~/Downloads/service-key.json worker:
  %li
    On both hosts I authenticated:
    .syntax
      %pre
        %code gcloud auth activate-service-account --key-file=service-key.json

%h2 Part 7:
%ul
  %li On mgr, I used gcloud shell commands to test publish-subscribe:
  %li
    .syntax
      %pre
        %code
          gcloud pubsub topics        create         topic10       --project pubsub-197323
          gcloud pubsub subscriptions create --topic topic10 sub10 --project pubsub-197323
          gcloud pubsub topics publish topic10 --message "hello0" --project pubsub-197323
          gcloud pubsub topics publish topic10 --message "hello1" --project pubsub-197323
          gcloud pubsub topics publish topic10 --message "hello2" --project pubsub-197323
          sleep 20
          gcloud pubsub subscriptions pull --auto-ack sub10 --project pubsub-197323
          gcloud pubsub subscriptions delete sub10          --project pubsub-197323
          gcloud pubsub topics        delete topic10        --project pubsub-197323


%h2 Part 8:
%ul
  %li
    The above test worked well so I moved on.
  %li
    Next, on mgr, I did this:
    vi ~/publish_tkrs.bash

  %li
    .syntax
      %pre
        =render 'lesson005publish_tkrs'

  %li
    On mgr I ran the above script:
    .syntax
      %pre
        %code bash ~/publish_tkrs.bash

%h2 Part 9:
%ul     
  %li
    Next, on worker, I did this:
    vi ~/workon_tkrs.bash
  %li
    .syntax
      %pre
        =render 'lesson005workon_tkrs0'
  %li
    Next, on worker, I did this:
    vi ~/workon_tkrs.py
  %li
    .syntax
      %pre
        =render 'lesson005workon_tkrs1'
  %li
    On worker I ran this:
    .syntax
      %pre
        %code bash ~/workon_tkrs.bash
  %li I found new predictions in my pubsub611 bucket!
  %li I saw them here:
  %li
    %a(href='https://console.cloud.google.com/storage' target='x')
      https://console.cloud.google.com/storage
  %li They looked like this:
  %li
    %img(src='/lesson005/z014.png')
  %li
    %img(src='/lesson005/z015.png')
%h2 Lesson Summary:
%ul
  %li Create GCP account
  %li Login to GCP
  %li Create Bucket: pubsub611
  %li Create Service Account: srvacct
  %li Get Service Account key: service-key.json
  %li Grant pubsub611 access to srvacct
  %li Create a kubernetes cluster of 2 Ubuntu nodes
  %li In each:
  %li
    %ul
      %li Install gcloud
      %li Install Anaconda
      %li Install service-key.json
      %li Use srvacct to login to gcloud
  %li On mgr create pub/sub topic and subsription
  %li test pub/sub
  %li On mgr publish tkrs
  %li On worker, work-on tkrs to generate predictions
  %li Copy predictions to bucket: pubsub611
  %li Look at predictions
  
%p
  %a(href='#top') [top]
%hr/
