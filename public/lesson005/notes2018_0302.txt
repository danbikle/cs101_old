~/cs101/public/lesson005/notes2018_0302.txt

This file descrbes my effort to walkthrough a blog post about gcp pub-sub and kubernetes:


http://willcrichton.net/notes/gcp-job-queue/

I noticed this syntax first:


# parallelize the computation over multiple cores:
from concurrent.futures import ProcessPoolExecutor
with ProcessPoolExecutor() as executor:
    list(executor.map(expensive_function, range(1000000)))

I searched:
https://www.google.com/search?q=In+Python+how+to+use+map+function

tried:

list(map(print, 'hello')) # works

def echo(x): return x     # works

list(map(echo, 'hello'))  # works

list(map(lambda x: x, 'hello')) # works:
# ['h', 'e', 'l', 'l', 'o']

q: in python 3, can I do this:
from concurrent.futures import ProcessPoolExecutor
?
yes!

I tried this:

# parallelize the computation over multiple cores:
from concurrent.futures import ProcessPoolExecutor
with ProcessPoolExecutor() as executor: list(executor.map(lambda x: x, range(10)))

# error:
# _pickle.PicklingError: Can't pickle <function <lambda> at 0x7fd2a132dd08>

I tried this:

# parallelize the computation over multiple cores:
from concurrent.futures import ProcessPoolExecutor
def echo(x): return x
with ProcessPoolExecutor() as executor: list(executor.map(echo, range(10)))

works! yay!!
>>> with ProcessPoolExecutor() as executor: list(executor.map(echo, range(10)))
... 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> 



I noticed links:

https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/
https://docs.docker.com/install/
https://github.com/willcrichton/gcp-job-queue


2018-03-06

I installed docker on p95:

https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-docker-ce-1

apt-get update

apt-get install apt-transport-https ca-certificates curl software-properties-common

/usr/bin/curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

apt-get update

apt-get install docker-ce

sudo docker run hello-world

groupadd docker

usermod -aG docker dan

systemctl enable docker

shutdown -r now

docker run hello-world

https://docs.docker.com/config/daemon/

google: daemon.json configuration file.

{
  "debug": true,
  "tls": true,
  "tlscert": "/var/docker/server.pem",
  "tlskey": "/var/docker/serverkey.pem",
  "hosts": ["tcp://192.168.59.3:2376"]
}


dockerd

or something like:

dockerd -D --tls=true --tlscert=/var/docker/server.pem --tlskey=/var/docker/serverkey.pem -H tcp://192.168.59.3:2376



I returned to:
http://willcrichton.net/notes/gcp-job-queue/

https://research.google.com/youtube8m/
https://groups.google.com/forum/#!forum/youtube8m-users


pip install -r requirements.txt

depends on...

https://github.com/willcrichton/gcp-job-queue

sudo bash

echo FROM ubuntu:16.04 > /tmp/ctxt/Dockerfile
cd /tmp/
docker build ctxt

docker image ls
docker container ls

docker rm cda6f5e55fd4 # removes container
docker image rm f2a91732366c

I should return to docker later...

I returned to:
http://willcrichton.net/notes/gcp-job-queue/

login to gcp

https://console.cloud.google.com/

create project: pubsub

check that this project can bill:
https://console.cloud.google.com/billing

Big Data > Pub/Sub.

https://console.cloud.google.com/cloudpubsub

enable API

create topic: topic10
  click on topic10
    create subscription: sub10

https://console.cloud.google.com/kubernetes

button create cluster

cluster of 1 [ master node ]

wait

click on it
click edit

click add node pool

Set Name to workers.
Set Preemptible nodes to Yes.
Set Autoscaling to On.
set Minimum size to 0.
Set Maximum size to your desired number of workers (we’ll use 3 for this example).
Then click Save.

wait

create bucket: pubsub611


on some host I need this:
# https://cloud.google.com/sdk/downloads
# https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-191.0.0-linux-x86_64.tar.gz
??

here is requirements.txt:

google-cloud-pubsub
google-cloud-monitoring
youtube-dl
pandas
tqdm


Can Python3 be used with these?


On some host,laptop even, I should have this:


# gcp-job-queue/master.py

# This script should help me with pubsub demo.
# Ref:
# http://willcrichton.net/notes/gcp-job-queue/
# https://github.com/willcrichton/gcp-job-queue

from google.cloud import pubsub
# How I pip install for google.cloud?
# ans:
# pip install -r requirements.txt

from tqdm import tqdm

#PROJECT = 'wc-personal'
#TOPIC = 'queue-example'

PROJECT = 'pubsub'
TOPIC   = 'topic10'

# Ref:
# https://github.com/willcrichton/gcp-job-queue/blob/master/youtube-ids
def main():
    with open('youtube-ids') as f:
        ids = [s.strip() for s in f.readlines()]

    publisher = pubsub.PublisherClient()
    topic = 'projects/{}/topics/{}'.format(PROJECT, TOPIC)
    for id in tqdm(ids):
        publisher.publish(topic, id)


if __name__ == '__main__':
    main()


master.py goes through every ID in our file: "youtube-ids"

Then it publishes them to the topic we chose earlier. You can just run this script from your laptop:

python master.py

Once master.py completes, your queue has been filled to the brim with IDs ready for downloading. Next, we need to create the script we’ll run on each worker:

oooooooooo

2018-03-07


created cluster
created host: tiny1
  apt-get install git
  git clone https://github.com/willcrichton/gcp-job-queue
  apt-get install -y python-pip curl
  pip install -r requirements.txt
    memory error
  enhance tiny1...
    added mem
    now pip works! yay!!
    
I should branch gcp-job-queue:

git checkout -b bikle


bikle101@tiny1:~/gcp-job-queue$ python master.py
100% 1000/1000 [00:00<00:00, 26715.14it/s]
bikle101@tiny1:~/gcp-job-queue$ 
bikle101@tiny1:~/gcp-job-queue$

I think it worked!
notice the fraction: 100% 1000/1000

